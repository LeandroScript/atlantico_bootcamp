# -*- coding: utf-8 -*-
"""Code2resolved

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rYHxCPHuY_eYGw42vE4Uwtb1keiKUdd8
"""

import nltk
nltk.download('punkt')

# Import necessary modules
from nltk.tokenize import word_tokenize, sent_tokenize
from nlp_utils import get_sample_Santo_Graal
import os
import sys
sys.path.append(os.getcwd().split('/NLP')[0])

# Split scene_one into sentences: sentences
scene_one = get_sample_Santo_Graal()
sentences = sent_tokenize(scene_one)
print(sentences[0])

# Use word_tokenize to tokenize the fourth sentence: tokenized_sent
tokenized_sent = word_tokenize(sentences[3])
print(tokenized_sent)

# Make a set of unique tokens in the entire scene: unique_tokens
unique_tokens = set(word_tokenize(scene_one))

# Print the unique tokens result
print(unique_tokens)