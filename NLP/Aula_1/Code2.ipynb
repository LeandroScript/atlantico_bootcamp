{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Code2resolved",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoiQoEZcFI8C",
        "outputId": "720496ca-2b1a-42c2-94da-e8b0b1715722"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hg4l8iBXL3HL"
      },
      "outputs": [],
      "source": [
        "# Import necessary modules\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nlp_utils import get_sample_Santo_Graal\n",
        "import os\n",
        "import sys\n",
        "sys.path.append(os.getcwd().split('/NLP')[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split scene_one into sentences: sentences\n",
        "scene_one = get_sample_Santo_Graal()\n",
        "sentences = sent_tokenize(scene_one)\n",
        "print(sentences[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5uz01R1NGX5o",
        "outputId": "0157891d-520e-4f4d-dc9e-f6b2f3eb2a38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SCENE 1: [wind] [clop clop clop] \n",
            " KING ARTHUR: Whoa there!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use word_tokenize to tokenize the fourth sentence: tokenized_sent\n",
        "tokenized_sent = word_tokenize(sentences[3])\n",
        "print(tokenized_sent)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpkN60JLGd3O",
        "outputId": "d397fbbd-3a1a-4d22-e3d3-8c4474564cd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ARTHUR', ':', 'It', 'is', 'I', ',', 'Arthur', ',', 'son', 'of', 'Uther', 'Pendragon', ',', 'from', 'the', 'castle', 'of', 'Camelot', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a set of unique tokens in the entire scene: unique_tokens\n",
        "unique_tokens = set(word_tokenize(scene_one))\n"
      ],
      "metadata": {
        "id": "QQRZM8zYGlNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the unique tokens result\n",
        "print(unique_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9xNoNqdGsHH",
        "outputId": "be74ec3f-a2ae-42d8-969f-2e6df48bed40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'1', 'No', 'get', 'using', 'point', 'not', 'here', 'halves', 'these', 'suggesting', 'interested', 'together', 'mean', 'my', 'winter', 'feathers', 'maybe', 'then', ',', 'under', 'why', \"n't\", \"'s\", 'So', 'climes', 'bangin', 'In', 'yet', 'knights', 'found', 'does', 'beat', 'line', 'weight', 'A', 'matter', 'velocity', 'you', 'or', 'Whoa', 'tropical', 'right', 'court', 'Halt', 'grips', 'south', 'trusty', 'join', 'a', 'temperate', 'maintain', 'horse', 'is', 'Oh', 'Camelot', 'must', 'go', 'speak', 'Pull', 'all', 'strangers', 'minute', 'sovereign', 'wings', 'You', 'an', 'ask', 'Listen', 'African', 'the', \"'\", 'carried', 'coconuts', 'Are', \"'d\", 'grip', 'pound', '2', '...', 'of', 'sun', 'plover', 'Wait', 'Arthur', 'but', 'there', 'length', 'since', 'King', 'its', 'What', '.', 'every', 'Will', 'wind', 'The', 'Well', 'It', 'They', 'migrate', '--', 'search', 'That', 'But', 'course', '!', 'and', 'to', 'this', 'yeah', 'son', 'Uther', \"'em\", 'SOLDIER', 'two', 'Saxons', 'your', 'tell', 'agree', 'are', 'anyway', 'dorsal', 'snows', 'European', 'non-migratory', 'Pendragon', 'they', 'house', '#', 'am', 'on', \"'ve\", 'zone', 'it', 'use', 'Who', 'back', 'will', 'We', 'bird', 'creeper', 'in', 'from', 'me', 'where', 'that', \"'re\", 'warmer', 'swallows', 'castle', 'who', 'carrying', 'land', 'empty', 'by', 'Court', 'wants', 'Mercea', 'with', 'held', 'second', '?', 'bring', 'Ridden', 'seek', 'five', 'goes', 'ounce', 'other', ']', 'he', 'servant', 'coconut', 'at', 'kingdom', 'Where', 'SCENE', 'them', 'Please', 'do', '[', 'guiding', 'if', 'just', 'ARTHUR', 'breadth', 'fly', 'order', 'Patsy', 'Not', 'strand', 'could', 'air-speed', 'through', 'master', 'covered', 'simple', 'forty-three', 'be', 'ratios', 'may', 'England', 'Am', 'Yes', 'martin', 'Supposing', ':', 'I', 'Found', 'defeator', 'our', 'question', \"'m\", 'times', 'husk', 'Britons', 'have', 'carry', 'needs', 'clop', 'swallow', 'lord', 'got', 'ridden', 'one', 'KING'}\n"
          ]
        }
      ]
    }
  ]
}