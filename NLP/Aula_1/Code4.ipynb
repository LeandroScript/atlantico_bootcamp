{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Code4resolved",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJaaNFkFGNa5",
        "outputId": "5b337426-ed1b-41b1-9081-370ebb10201f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['#nlp', '#python']\n",
            "['@Atlantico', '#nlp', '#python']\n",
            "[['This', 'is', 'the', 'best', '#nlp', 'exercise', 'ive', 'found', 'online', '!', '#python'], ['#NLP', 'is', 'super', 'fun', '!', '<3', '#learning'], ['Thanks', '@Atlantico', ':)', '#nlp', '#python']]\n"
          ]
        }
      ],
      "source": [
        "#Import the necessary modules\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.tokenize import regexp_tokenize\n",
        "from nlp_utils import get_tweets_sample\n",
        "\n",
        "\n",
        "tweets = get_tweets_sample()\n",
        "\n",
        "# Define a regex pattern to find hashtags: pattern1\n",
        "pattern1=r\"#\\w+\"\n",
        "# Use the pattern on the first tweet in the tweets list\n",
        "hashtags=tweets[0]\n",
        "print(regexp_tokenize(hashtags, pattern1))\n",
        "\n",
        "# Write a pattern that matches both mentions (@) and hashtags\n",
        "pattern2=r\"([#|@]\\w+)\"\n",
        "# Use the pattern on the last tweet in the tweets list\n",
        "mentions_hashtags=tweets[-1]\n",
        "print(regexp_tokenize(mentions_hashtags, pattern2))\n",
        "\n",
        "# Use the TweetTokenizer to tokenize all tweets into one list\n",
        "tknzr=TweetTokenizer()\n",
        "all_tokens=[tknzr.tokenize(t) for t in tweets]\n",
        "print(all_tokens)\n"
      ]
    }
  ]
}